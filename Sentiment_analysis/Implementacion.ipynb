{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SENTIMENT ANALYSIS\n",
    "\n",
    "Aqui se llevara a cabo el analisis de sentiemiento de las reviews, utilizando el modelo creado en el archivo NLP.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la ruta actual del directorio, nos servira para cargar y guardar, los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_actual = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos el modelo creado en NPL, llamado NPL_SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_modelo = os.path.join(ruta_actual, 'modelo', 'NPL_SVC.pkl')\n",
    "modelo_svc = joblib.load(ruta_modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos el dataframe con el que vamos a trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ruta_df = r'C:\\PROYECTO-INDIVIDUAL\\Data\\parquet\\reviews.parquet'\n",
    "df_procesar = pd.read_parquet(ruta_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos con normalizar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Importamos las stopwords y el filtro a utilizar\n",
    "stopwords_ingles = nltk.corpus.stopwords.words('english')\n",
    "stopwords_spanish = nltk.corpus.stopwords.words('spanish')\n",
    "stopwords_russian = nltk.corpus.stopwords.words('russian')\n",
    "stopwords_portuguese = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwords = stopwords_ingles + stopwords_spanish + stopwords_russian + stopwords_portuguese\n",
    "filtrar = []\n",
    "if True:\n",
    "    filtrar.append('game')\n",
    "    filtrar.append('play')\n",
    "    filtrar.append('games')\n",
    "    filtrar.append('playing')\n",
    "\n",
    "# Instanciamos un wordnetlemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Creamos un diccionario con las tags de las clases de palabras que usaremos al lematizar\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Creamos una lista de recomendaciones lematizadas\n",
    "recomendaciones_lemm = []\n",
    "for recomendacion in df_procesar.review:\n",
    "    recomendacion = re.sub(\"[^a-zA-Z]\",\" \",str(recomendacion))\n",
    "    recomendacion = recomendacion.lower()\n",
    "    recomendacion = nltk.word_tokenize(recomendacion)\n",
    "    recomendacion = [palabra for palabra in recomendacion if len(palabra)>2]\n",
    "    recomendacion = [palabra for palabra in recomendacion if not palabra in stopwords]\n",
    "    recomendacion = [wordnet_lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in recomendacion]\n",
    "    recomendacion = \" \".join(recomendacion)\n",
    "    recomendaciones_lemm.append(recomendacion)\n",
    "\n",
    "# Creamos una columna con las reviews lematizadas\n",
    "df_procesar['review_lemm'] = recomendaciones_lemm\n",
    "\n",
    "# Creamos una lista con los valores de las recomendaciones\n",
    "lista_recomendaciones = list(df_procesar['review_lemm'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Asignamos el max_features\n",
    "max_features = 3000\n",
    "\n",
    "# Creamos el vectorizador\n",
    "cou_vec = CountVectorizer(max_features=max_features, stop_words=\"english\", ngram_range=(1,2))\n",
    "\n",
    "# Creamos una matriz con la lista de recomendaciones de manera vectorizada\n",
    "matriz_reviews = cou_vec.fit_transform(lista_recomendaciones)\n",
    "\n",
    "# Transformamos en array la matriz\n",
    "x = matriz_reviews.toarray()\n",
    "\n",
    "# Hacer predicciones con los datos\n",
    "predicciones = modelo_svc.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez predichos los resultados, asignamos esas predicciones a una nueva columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Guardamos en columna nueva\n",
    "df_procesar['sentiment_analysis'] = predicciones\n",
    "\n",
    "# Dropeamos columnas innecesarias\n",
    "df_procesar.drop(columns=['review', 'review_lemm'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el df actualizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ruta_guardar = r'C:\\PROYECTO-INDIVIDUAL\\Data\\parquet\\review_predicha.parquet'\n",
    "df_procesar.to_parquet(ruta_guardar, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
